{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-C8F58JN:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1618297744768)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3d55a19a\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession\n",
    ".builder\n",
    ".appName(\"SparkSQLExampleApp\")\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".option(\"header\", \"true\")\n",
    ".load(\"C:/Users/GAME/Desktop/departuredelays.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT distance, origin, destination\n",
    "FROM us_delay_flights_tbl WHERE distance > 1000\n",
    "ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "FROM us_delay_flights_tbl\n",
    "WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "ORDER by delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "CASE\n",
    "WHEN delay > 360 THEN 'Very Long Delays'\n",
    "WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "WHEN delay = 0 THEN 'No Delays'\n",
    "ELSE 'Early'\n",
    "END AS Flight_Delays\n",
    "FROM us_delay_flights_tbl\n",
    "ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "2: error: unclosed string literal\r",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: unclosed string literal\r",
      "       spark.sql(\"CREATE TABLE 'managed_us_delay_flights_tbl' (date STRING, delay INT,\r",
      "                 ^\r",
      "<console>:3: error: unclosed string literal\r",
      "       distance INT, origin STRING, destination STRING)\")\r",
      "                                                       ^\r",
      ""
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT,\n",
    "distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: String = date STRING, delay INT, distance INT, origin STRING, destination STRING\r\n",
       "csv_file: String = C:/Users/GAME/Desktop/departuredelays.csv\r\n",
       "flights_df: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "val csv_file = \"C:/Users/GAME/Desktop/departuredelays.csv\"\n",
    "val flights_df = spark.read.csv(csv_file)\n",
    "flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT,\n",
    "distance INT, origin STRING, destination STRING)\n",
    "USING csv OPTIONS (PATH\n",
    "'C:/Users/GAME/Desktop/departuredelays.csv')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_sfo: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_sfo = spark.sql(\"\"\"SELECT date, delay, origin, destination FROM\n",
    "us_delay_flights_tbl WHERE origin = \"SFO\"\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_jfk: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_jfk = spark.sql(\"\"\"SELECT date, delay, origin, destination FROM\n",
    "us_delay_flights_tbl WHERE origin = \"JFK\"\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: Boolean = true\r\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Column] = [name: string, description: string ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()\n",
    "spark.catalog.listTables()\n",
    "spark.catalog.listColumns(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usFlightsDF: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 3 more fields]\r\n",
       "usFlightsDF2: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val usFlightsDF = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\n",
    "val usFlightsDF2 = spark.table(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "|1031245|   -4|     602|   ABE|        ATL|\n",
      "|1030605|    0|     602|   ABE|        ATL|\n",
      "|1041243|   10|     602|   ABE|        ATL|\n",
      "|1040605|   28|     602|   ABE|        ATL|\n",
      "|1051245|   88|     602|   ABE|        ATL|\n",
      "|1050605|    9|     602|   ABE|        ATL|\n",
      "|1061215|   -6|     602|   ABE|        ATL|\n",
      "|1061725|   69|     602|   ABE|        ATL|\n",
      "|1061230|    0|     369|   ABE|        DTW|\n",
      "|1060625|   -3|     602|   ABE|        ATL|\n",
      "|1070600|    0|     369|   ABE|        DTW|\n",
      "|1071725|    0|     602|   ABE|        ATL|\n",
      "|1071230|    0|     369|   ABE|        DTW|\n",
      "|1070625|    0|     602|   ABE|        ATL|\n",
      "|1071219|    0|     569|   ABE|        ORD|\n",
      "|1080600|    0|     369|   ABE|        DTW|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: String = C:/Users/GAME/Desktop/parquet/2010-summary.parquet/\r\n",
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"\"\"C:/Users/GAME/Desktop/parquet/2010-summary.parquet/\"\"\"\n",
    "val df = spark.read.format(\"parquet\").load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "31: error: value asd is not a member of Unit\r",
     "output_type": "error",
     "traceback": [
      "<console>:31: error: value asd is not a member of Unit\r",
      "       .save(\"/tmp/data/parquet/df_parquet\")asd\r",
      "                                            ^\r",
      ""
     ]
    }
   ],
   "source": [
    "df.write.format(\"parquet\")\n",
    ".mode(\"overwrite\")\n",
    ".save(\"/tmp/data/parquet/df_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "2: error: ';' expected but ':' found.\r",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: ';' expected but ':' found.\r",
      "       import org.apache.spark:spark-avro_2.12:\r",
      "                              ^\r",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark:spark-avro_2.12:3.1.1\n",
    "val file = \"C:/Users/GAME/Desktop/avro/*\"\n",
    "val df = spark.read.format(\"avro\").load(file)\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")\n",
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()\n",
    "df.write.format(\"avro\")\n",
    ".mode(\"overwrite\")\n",
    ".save(\"C:/Users/GAME/Desktop/prueba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "25: error: object avro is not a member of package org.apache.spark.sql\r",
     "output_type": "error",
     "traceback": [
      "<console>:25: error: object avro is not a member of package org.apache.spark.sql\r",
      "       import org.apache.spark.sql.avro.functions._\r",
      "                                   ^\r",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.avro.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ciudad: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [date: int, delay: int ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ciudad = df.select(\"date\", \"delay\", \"origin\", \"destination\").where(\"delay > 120 and origin = 'SFO' and destination = 'ORD'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|date   |delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|1011410|124  |SFO   |ORD        |\n",
      "|1022330|326  |SFO   |ORD        |\n",
      "|1021410|190  |SFO   |ORD        |\n",
      "|1101410|184  |SFO   |ORD        |\n",
      "|1190925|297  |SFO   |ORD        |\n",
      "|1241110|139  |SFO   |ORD        |\n",
      "|1301800|167  |SFO   |ORD        |\n",
      "|1011237|122  |SFO   |ORD        |\n",
      "|1032258|163  |SFO   |ORD        |\n",
      "|1031920|193  |SFO   |ORD        |\n",
      "|1031755|396  |SFO   |ORD        |\n",
      "|1071040|279  |SFO   |ORD        |\n",
      "|1161210|225  |SFO   |ORD        |\n",
      "|1221040|215  |SFO   |ORD        |\n",
      "|1261104|258  |SFO   |ORD        |\n",
      "|1020720|145  |SFO   |ORD        |\n",
      "|1021205|154  |SFO   |ORD        |\n",
      "|1031550|131  |SFO   |ORD        |\n",
      "|1041205|126  |SFO   |ORD        |\n",
      "|1051205|320  |SFO   |ORD        |\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "ciudad.show(20, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "32: error: value when is not a member of org.apache.spark.sql.DataFrame\r",
     "output_type": "error",
     "traceback": [
      "<console>:32: error: value when is not a member of org.apache.spark.sql.DataFrame\r",
      "possible cause: maybe a semicolon is missing before `value when'?\r",
      "       .when(col(\"delay\") > 360, \"Very Long Delays\")\r",
      "        ^\r",
      "<console>:33: error: not found: value delay\r",
      "       .when(col(\"delay\") > 120 && col (delay) < 360, \"Long Delays\")\r",
      "                                        ^\r",
      "<console>:34: error: not found: value delay\r",
      "       .when(col(\"delay\") > 60 && col (delay) < 120, \"Short Delays\")\r",
      "                                       ^\r",
      "<console>:35: error: not found: value delay\r",
      "       .when(col(\"delay\") > 0 && col (delay) < 60, \"Tolerable Delays\")\r",
      "                                      ^\r",
      "<console>:38: error: too many arguments (2) for method desc: (columnName: String)org.apache.spark.sql.Column\r",
      "       .orderBy(desc(\"origin\", \"delay\"))\r",
      "                               ^\r",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{when, _}\n",
    "import spark.sqlContext.implicits._\n",
    "val ciudad = df.select(\"delay\", \"origin\", \"destination\")\n",
    ".when(col(\"delay\") > 360, \"Very Long Delays\")\n",
    ".when(col(\"delay\") > 120 && col (delay) < 360, \"Long Delays\")\n",
    ".when(col(\"delay\") > 60 && col (delay) < 120, \"Short Delays\")\n",
    ".when(col(\"delay\") > 0 && col (delay) < 60, \"Tolerable Delays\")\n",
    ".when(col(\"delay\") === 0, \"No Delays\")\n",
    ".otherwise(\"Early\")\n",
    ".orderBy(desc(\"origin\", \"delay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "27: error: type withColumn is not a member of org.apache.spark.sql.DataFrame\r",
     "output_type": "error",
     "traceback": [
      "<console>:27: error: type withColumn is not a member of org.apache.spark.sql.DataFrame\r",
      "       val nueva_fecha = (new df.withColumn(\"fecha\", to_timestamp(col(\"date\"), \"MM/dd/yyyy\")). drop(\"date\"))\r",
      "                                 ^\r",
      ""
     ]
    }
   ],
   "source": [
    "val nueva_fecha = (new df.withColumn(\"fecha\", to_timestamp(col(\"date\"), \"MM/dd/yyyy\")). drop(\"date\"))\n",
    "nueva_fecha.select(\"DAY(fecha)\", \"MONTH(fecha)\", \"SUM(delay)\").groupBy(\"DAY(fecha)\", \"MONTH(fecha)\")\n",
    "spark.sql(\"SELECT DAY(fecha), MONTH(fecha), SUM(delay) from us_delay_flights_tbl group by DAY(fecha), MONTH(fecha)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: String = C:/Users/GAME/Desktop/json/*\r\n",
       "df: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"C:/Users/GAME/Desktop/json/*\"\n",
    "val df = spark.read.format(\"csv\").load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+\n",
      "|                 _c0|                 _c1|         _c2|\n",
      "+--------------------+--------------------+------------+\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...| \"count\":15}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...|  \"count\":1}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...|\"count\":344}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...| \"count\":15}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...| \"count\":62}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...|  \"count\":1}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...| \"count\":62}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...|\"count\":588}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...| \"count\":40}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...|  \"count\":1}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...|\"count\":325}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...| \"count\":39}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...| \"count\":64}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...|  \"count\":1}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...| \"count\":41}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...| \"count\":30}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...|  \"count\":6}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...|  \"count\":4}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...|\"count\":230}|\n",
      "|{\"ORIGIN_COUNTRY_...|\"DEST_COUNTRY_NAM...|  \"count\":1}|\n",
      "+--------------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file: String = C:/Users/GAME/Desktop/json/*\r\n",
       "df: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"C:/Users/GAME/Desktop/json/*\"\n",
    "val df = spark.read.format(\"csv\").load(file)\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")\n",
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()\n",
    "df.write.format(\"json\")\n",
    ".mode(\"overwrite\")\n",
    ".save(\"C:/Users/GAME/Desktop/prueba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|                 _c0|                _c1|  _c2|\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file: String = C:/Users/GAME/Desktop/csv/*\r\n",
       "df: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"C:/Users/GAME/Desktop/csv/*\"\n",
    "val df = spark.read.format(\"csv\").load(file)\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")\n",
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()\n",
    "df.write.format(\"csv\")\n",
    ".mode(\"overwrite\")\n",
    ".save(\"C:/Users/GAME/Desktop/prueba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val file = \"C:/Users/GAME/Desktop/orc/*\"\n",
    "val df = spark.read.format(\"orc\").load(file)\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")\n",
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()\n",
    "df.write.format(\"orc\")\n",
    ".mode(\"overwrite\")\n",
    ".save(\"C:/Users/GAME/Desktop/prueba\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
