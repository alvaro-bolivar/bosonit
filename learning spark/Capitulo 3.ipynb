{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-C8F58JN:4041\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1618215432845)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.avg\r\n",
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@703457e8\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.avg\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession\n",
    ".builder\n",
    ".appName(\"AuthorsAges\")\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataDF: org.apache.spark.sql.DataFrame = [name: string, age: int]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataDF = spark.createDataFrame(Seq((\"Brooke\", 20), (\"Brooke\", 25),\n",
    "(\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35))).toDF(\"name\", \"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgDF: org.apache.spark.sql.DataFrame = [name: string, avg(age): double]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val avgDF = dataDF.groupBy(\"name\").agg(avg(\"age\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Brooke|    22.5|\n",
      "| Jules|    30.0|\n",
      "|    TD|    35.0|\n",
      "| Denny|    31.0|\n",
      "+------+--------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "avgDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\r\n",
       "import org.apache.spark.sql.types._\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nameTypes: org.apache.spark.sql.types.StringType.type = StringType\r\n",
       "FirstName: org.apache.spark.sql.types.StringType.type = StringType\r\n",
       "lastName: org.apache.spark.sql.types.StringType.type = StringType\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nameTypes = StringType\n",
    "val FirstName = nameTypes\n",
    "val lastName = nameTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(author,StringType,false), StructField(title,StringType,false), StructField(pages,IntegerType,false))\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = StructType(Array(StructField(\"author\", StringType, false),\n",
    "StructField(\"title\", StringType, false),\n",
    "StructField(\"pages\", IntegerType, false)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: String = author STRING, title STRING, pages INT\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = \"author STRING, title STRING, pages INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(Id,IntegerType,false), StructField(First,StringType,false), StructField(Last,StringType,false), StructField(Url,StringType,false), StructField(Published,StringType,false), StructField(Hits,IntegerType,false), StructField(Campaigns,ArrayType(StringType,true),false))\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = StructType(Array(StructField(\"Id\", IntegerType, false),\n",
    "StructField(\"First\", StringType, false),\n",
    "StructField(\"Last\", StringType, false),\n",
    "StructField(\"Url\", StringType, false),\n",
    "StructField(\"Published\", StringType, false),\n",
    "StructField(\"Hits\", IntegerType, false),\n",
    "StructField(\"Campaigns\", ArrayType(StringType), false)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blogsDF: org.apache.spark.sql.DataFrame = [Id: int, First: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val blogsDF = spark.read.schema(schema).json(\"C:/Users/GAME/Desktop/blogs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+----------------------------+\n",
      "|Id |First    |Last   |Url              |Published|Hits |Campaigns                   |\n",
      "+---+---------+-------+-----------------+---------+-----+----------------------------+\n",
      "|1  |Jules    |Damji  |https://tinyurl.1|1/4/2016 |4535 |[twitter, LinkedIn]         |\n",
      "|2  |Brooke   |Wenig  |https://tinyurl.2|5/5/2018 |8908 |[twitter, LinkedIn]         |\n",
      "|3  |Denny    |Lee    |https://tinyurl.3|6/7/2019 |7659 |[web, twitter, FB, LinkedIn]|\n",
      "|4  |Tathagata|Das    |https://tinyurl.4|5/12/2018|10568|[twitter, FB]               |\n",
      "|5  |Matei    |Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB, LinkedIn]|\n",
      "|6  |Reynold  |Xin    |https://tinyurl.6|3/2/2015 |25568|[twitter, LinkedIn]         |\n",
      "+---+---------+-------+-----------------+---------+-----+----------------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "blogsDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "()\n",
      "StructType(StructField(Id,IntegerType,true), StructField(First,StringType,true), StructField(Last,StringType,true), StructField(Url,StringType,true), StructField(Published,StringType,true), StructField(Hits,IntegerType,true), StructField(Campaigns,ArrayType(StringType,true),true))\n"
     ]
    }
   ],
   "source": [
    "println(blogsDF.printSchema)\n",
    "println(blogsDF.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[String] = Array(Id, First, Last, Url, Published, Hits, Campaigns)\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogsDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: org.apache.spark.sql.Column = Id\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogsDF.col(\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "blogsDF.select(col(\"Hits\") * 2).show(2)\n",
    "blogsDF.select(expr(\"Hits * 2\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|Big Hitters|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|      false|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|      false|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|      false|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|       true|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|       true|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|       true|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "blogsDF.withColumn(\"Big Hitters\", col(\"Hits\") > 10000).show()\n",
    "blogsDF.withColumn(\"Big Hitters\", expr(\"Hits > 10000\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    AuthorsId|\n",
      "+-------------+\n",
      "|  JulesDamji1|\n",
      "| BrookeWenig2|\n",
      "|    DennyLee3|\n",
      "|TathagataDas4|\n",
      "+-------------+\n",
      "only showing top 4 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "blogsDF.withColumn(\"AuthorsId\", (concat(expr(\"First\"), expr(\"Last\"), expr(\"Id\")))).select(col(\"AuthorsId\")).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "blogsDF.select(expr(\"Hits\")).show(2)\n",
    "blogsDF.select(col(\"Hits\")).show(2)\n",
    "blogsDF.select(\"Hits\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "blogsDF.sort(col(\"Id\").desc).show()\n",
    "blogsDF.sort($\"Id\".desc).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blogRow: org.apache.spark.sql.Row = [6,Reynold,Xin,https://tinyurl.6,255568,3/2/2015,[Ljava.lang.String;@10a70dfc]\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val blogRow = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\",\n",
    "Array(\"twitter\", \"LinkedIn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Any = Reynold\r\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogRow(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rows: Seq[(String, String)] = List((Matei Zaharia,CA), (Reynold Xin,CA))\r\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rows = Seq((\"Matei Zaharia\", \"CA\"), (\"Reynold Xin\", \"CA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "authorsDF: org.apache.spark.sql.DataFrame = [Author: string, State: string]\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val authorsDF = rows.toDF(\"Author\", \"State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|       Author|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "authorsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fireSchema: org.apache.spark.sql.types.StructType = StructType(StructField(CallNumber,IntegerType,true), StructField(UnitID,StringType,true), StructField(IncidentNumber,IntegerType,true), StructField(CallType,StringType,true), StructField(CallDate,StringType,true), StructField(WatchDate,StringType,true), StructField(CallFinalDisposition,StringType,true), StructField(AvailableDtTm,StringType,true), StructField(Address,StringType,true), StructField(City,StringType,true), StructField(Zipcode,IntegerType,true), StructField(Battalion,StringType,true), StructField(StationArea,StringType,true), StructField(Box,StringType,true), StructField(OriginalPriority,StringType,true), StructField(Priority,StringType,true), StructField(FinalPriority,IntegerType,true), StructField(ALSUni,BooleanType,true),...\r\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fireSchema = StructType(Array(StructField(\"CallNumber\", IntegerType, true), StructField(\"UnitID\", StringType, true),\n",
    "StructField(\"IncidentNumber\", IntegerType, true),\n",
    "StructField(\"CallType\", StringType, true),\n",
    "StructField(\"CallDate\", StringType, true),\n",
    "StructField(\"WatchDate\", StringType, true),\n",
    "StructField(\"CallFinalDisposition\", StringType, true),\n",
    "StructField(\"AvailableDtTm\", StringType, true),\n",
    "StructField(\"Address\", StringType, true),\n",
    "StructField(\"City\", StringType, true),\n",
    "StructField(\"Zipcode\", IntegerType, true),\n",
    "StructField(\"Battalion\", StringType, true),\n",
    "StructField(\"StationArea\", StringType, true),\n",
    "StructField(\"Box\", StringType, true),\n",
    "StructField(\"OriginalPriority\", StringType, true),\n",
    "StructField(\"Priority\", StringType, true),\n",
    "StructField(\"FinalPriority\", IntegerType, true),\n",
    "StructField(\"ALSUni\", BooleanType, true),\n",
    "StructField(\"CallTypeGroup\", StringType, true),\n",
    "StructField(\"NumAlarms\", IntegerType, true),\n",
    "StructField(\"UnitType\", StringType, true),\n",
    "StructField(\"UnitSequenceInCallDispatch\", IntegerType, true),\n",
    "StructField(\"FirePreventionDistrict\", StringType, true),\n",
    "StructField(\"SupervisorDistrict\", StringType, true),\n",
    "StructField(\"Neighborhood\", StringType, true),\n",
    "StructField(\"Location\", StringType, true),\n",
    "StructField(\"RowID\", StringType, true),\n",
    "StructField(\"Delay\", FloatType, true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sfFireFile: String = C:/Users/GAME/Desktop/sf-fire-calls.csv\r\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sfFireFile=\"C:/Users/GAME/Desktop/sf-fire-calls.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fireDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fireDF = spark.read.schema(fireSchema).csv(\"C:/Users/GAME/Desktop/sf-fire-calls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|null          |AvailableDtTm         |CallType      |\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fewFireDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [IncidentNumber: int, AvailableDtTm: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fewFireDF = fireDF\n",
    ".select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    ".where($\"CallType\" =!= \"Medical Incident\")\n",
    "fewFireDF.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "2: error: ';' expected but '.' found.\r",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: ';' expected but '.' found.\r",
      "       from pyspark.sql.functions import *\r",
      "                   ^\r",
      ""
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DistinctCallTypes|\n",
      "+-----------------+\n",
      "|               31|\n",
      "+-----------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireDF\n",
    ".select(\"CallType\")\n",
    ".where(col(\"CallType\").isNotNull)\n",
    ".agg(countDistinct('CallType) as 'DistinctCallTypes)\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|CallType                           |\n",
      "+-----------------------------------+\n",
      "|Elevator / Escalator Rescue        |\n",
      "|Marine Fire                        |\n",
      "|Aircraft Emergency                 |\n",
      "|Confined Space / Structure Collapse|\n",
      "|CallType                           |\n",
      "|Administrative                     |\n",
      "|Alarms                             |\n",
      "|Odor (Strange / Unknown)           |\n",
      "|Citizen Assist / Service Call      |\n",
      "|HazMat                             |\n",
      "+-----------------------------------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireDF\n",
    ".select(\"CallType\")\n",
    ".where(col(\"CallType\").isNotNull)\n",
    ".distinct()\n",
    ".show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|5.35                 |\n",
      "|6.25                 |\n",
      "|5.2                  |\n",
      "|5.6                  |\n",
      "|7.25                 |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newFireDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newFireDF = fireDF.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "newFireDF\n",
    ".select(\"ResponseDelayedinMins\")\n",
    ".where(col(\"ResponseDelayedinMins\") > 5)\n",
    ".show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|null               |null               |null               |\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fireTsDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fireTsDF = newFireDF\n",
    ".withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    ".drop(\"CallDate\")\n",
    ".withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    ".drop(\"WatchDate\")\n",
    ".withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\n",
    "\"MM/dd/yyyy hh:mm:ss a\"))\n",
    ".drop(\"AvailableDtTm\")\n",
    "\n",
    "fireTsDF\n",
    ".select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",
    ".show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              null|\n",
      "|              2000|\n",
      "|              2001|\n",
      "|              2002|\n",
      "|              2003|\n",
      "|              2004|\n",
      "|              2005|\n",
      "|              2006|\n",
      "|              2007|\n",
      "|              2008|\n",
      "|              2009|\n",
      "|              2010|\n",
      "|              2011|\n",
      "|              2012|\n",
      "|              2013|\n",
      "|              2014|\n",
      "|              2015|\n",
      "|              2016|\n",
      "|              2017|\n",
      "|              2018|\n",
      "+------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTsDF\n",
    ".select(year($\"IncidentDate\"))\n",
    ".distinct()\n",
    ".orderBy(year($\"IncidentDate\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------+\n",
      "|CallType                       |count |\n",
      "+-------------------------------+------+\n",
      "|Medical Incident               |113794|\n",
      "|Structure Fire                 |23319 |\n",
      "|Alarms                         |19406 |\n",
      "|Traffic Collision              |7013  |\n",
      "|Citizen Assist / Service Call  |2524  |\n",
      "|Other                          |2166  |\n",
      "|Outside Fire                   |2094  |\n",
      "|Vehicle Fire                   |854   |\n",
      "|Gas Leak (Natural and LP Gases)|764   |\n",
      "|Water Rescue                   |755   |\n",
      "+-------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTsDF\n",
    ".select(\"CallType\")\n",
    ".where(col(\"CallType\").isNotNull)\n",
    ".groupBy(\"CallType\")\n",
    ".count()\n",
    ".orderBy(desc(\"count\"))\n",
    ".show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|sum(NumAlarms)|avg(ResponseDelayedinMins)|min(ResponseDelayedinMins)|max(ResponseDelayedinMins)|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|        176170|         3.892364154521585|               0.016666668|                   1844.55|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTsDF\n",
    ".select(sum(\"NumAlarms\"), avg(\"ResponseDelayedinMins\"),\n",
    "min(\"ResponseDelayedinMins\"), max(\"ResponseDelayedinMins\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row: org.apache.spark.sql.Row = [350,true,Learning Spark 2E,null]\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val row = Row(350, true, \"Learning Spark 2E\", null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Any = Learning Spark 2E\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class DeviceIoTData\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class DeviceIoTData (battery_level: Long, c02_level: Long,\n",
    "cca2: String, cca3: String, cn: String, device_id: Long,\n",
    "device_name: String, humidity: Long, ip: String, latitude: Double,\n",
    "lcd: String, longitude: Double, scale:String, temp: Long,\n",
    "timestamp: Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ds: org.apache.spark.sql.Dataset[DeviceIoTData] = [battery_level: bigint, c02_level: bigint ... 13 more fields]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds = spark.read.json(\"C:/Users/GAME/Desktop/iot_devices.json\").as[DeviceIoTData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----+----+-------------+---------+---------------------+--------+-------------+--------+------+---------+-------+----+-------------+\n",
      "|battery_level|c02_level|cca2|cca3|cn           |device_id|device_name          |humidity|ip           |latitude|lcd   |longitude|scale  |temp|timestamp    |\n",
      "+-------------+---------+----+----+-------------+---------+---------------------+--------+-------------+--------+------+---------+-------+----+-------------+\n",
      "|8            |868      |US  |USA |United States|1        |meter-gauge-1xbYRYcj |51      |68.161.225.1 |38.0    |green |-97.0    |Celsius|34  |1458444054093|\n",
      "|7            |1473     |NO  |NOR |Norway       |2        |sensor-pad-2n2Pea    |70      |213.161.254.1|62.47   |red   |6.15     |Celsius|11  |1458444054119|\n",
      "|2            |1556     |IT  |ITA |Italy        |3        |device-mac-36TWSKiT  |44      |88.36.5.1    |42.83   |red   |12.83    |Celsius|19  |1458444054120|\n",
      "|6            |1080     |US  |USA |United States|4        |sensor-pad-4mzWkz    |32      |66.39.173.154|44.06   |yellow|-121.32  |Celsius|28  |1458444054121|\n",
      "|4            |931      |PH  |PHL |Philippines  |5        |therm-stick-5gimpUrBB|62      |203.82.41.9  |14.58   |green |120.97   |Celsius|25  |1458444054122|\n",
      "+-------------+---------+----+----+-------------+---------+---------------------+--------+-------------+--------+------+---------+-------+----+-------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "ds.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterTempDS: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [device_name: string, device_id: bigint ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filterTempDS = ds.select($\"device_name\", $\"device_id\", $\"temp\", $\"humidity\").where(\"temp > 30 and humidity > 70\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------+----+--------+\n",
      "|device_name           |device_id|temp|humidity|\n",
      "+----------------------+---------+----+--------+\n",
      "|meter-gauge-17zb8Fghhl|17       |31  |98      |\n",
      "|sensor-pad-48jt4eL    |48       |31  |97      |\n",
      "|sensor-pad-5410CWPrNb6|54       |34  |73      |\n",
      "|device-mac-63GL4xSaZbj|63       |31  |91      |\n",
      "|device-mac-81nsKomrRe |81       |31  |90      |\n",
      "+----------------------+---------+----+--------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "filterTempDS.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class DeviceTempByCountry\r\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class DeviceTempByCountry(temp: Long, device_name: String, device_id: Long,\n",
    "cca3: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dsTemp2: org.apache.spark.sql.Dataset[DeviceTempByCountry] = [temp: bigint, device_name: string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dsTemp2 = ds.select($\"temp\", $\"device_name\", $\"device_id\", $\"device_id\", $\"cca3\").where(\"temp > 25\").as[DeviceTempByCountry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+---------+---------+----+\n",
      "|temp|device_name          |device_id|device_id|cca3|\n",
      "+----+---------------------+---------+---------+----+\n",
      "|34  |meter-gauge-1xbYRYcj |1        |1        |USA |\n",
      "|28  |sensor-pad-4mzWkz    |4        |4        |USA |\n",
      "|27  |sensor-pad-6al7RTAobR|6        |6        |USA |\n",
      "|27  |sensor-pad-8xUD6pzsQI|8        |8        |JPN |\n",
      "|26  |sensor-pad-10BsywSYUF|10       |10       |USA |\n",
      "+----+---------------------+---------+---------+----+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "dsTemp2.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----+----+-------------+---------+--------------------+--------+------------+--------+-----+---------+-------+----+-------------+\n",
      "|battery_level|c02_level|cca2|cca3|cn           |device_id|device_name         |humidity|ip          |latitude|lcd  |longitude|scale  |temp|timestamp    |\n",
      "+-------------+---------+----+----+-------------+---------+--------------------+--------+------------+--------+-----+---------+-------+----+-------------+\n",
      "|8            |868      |US  |USA |United States|1        |meter-gauge-1xbYRYcj|51      |68.161.225.1|38.0    |green|-97.0    |Celsius|34  |1458444054093|\n",
      "+-------------+---------+----+----+-------------+---------+--------------------+--------+------------+--------+-----+---------+-------+----+-------------+\n",
      "only showing top 1 row\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "ds.show(1, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.write.format(\"parquet\").save(\"C:/Users/GAME/Desktop/ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\".\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\".\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:676)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:743)\r",
      "  at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:993)\r",
      "  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:311)\r",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\r",
      "  ... 37 elided\r",
      ""
     ]
    }
   ],
   "source": [
    "ds.write.format(\"json\").save(\"C:/Users/GAME/Desktop/ds_json\")\n",
    "ds.write.format(\"csv\").save(\"C:/Users/GAME/Desktop/ds_csv\")\n",
    "ds.write.format(\"avro\").save(\"C:/Users/GAME/Desktop/ds_avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dspart: org.apache.spark.sql.Dataset[DeviceIoTData] = [battery_level: bigint, c02_level: bigint ... 13 more fields]\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dspart = ds.repartition(partitionExprs = col(\"battery_level\"), numPartitions = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspart.write.format(\"json\").save(\"C:/Users/GAME/Desktop/dspart_json\")\n",
    "dspart.write.format(\"csv\").save(\"C:/Users/GAME/Desktop/dspart_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
